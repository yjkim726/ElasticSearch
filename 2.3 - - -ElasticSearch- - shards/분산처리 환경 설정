

1 Elasticsearch Cluster 구성

 
◾단일 서버 분산 환경
단일 서버에서 n 개의 노드를 하나의 클러스터로 구성


◾다중 서버 분산 노드 환경
한 대 이상의 여러 서버들을 하나의 클러스터로 구성
> 각각 IP를 다르게 하여 노드별 구분



◾현재 Athena1 / Athena2 클러스터 구성 상황





◾설정시 주의사항
-  elasticsearch 하위 디렉토리에 있는 data 경로는 꼭 설정해줘야 한다 ( x-pack 설치 이후에 환경설정 변경 하려고 할 경우 경로가 꼬여서 제대로 설정이 안됨 )
-  elasticsearch tar 푼 디렉토리를 cp 로 복사해서 사용할 경우에도 기존 data 디렉토리에 있는 내용을 지워준다


◾오류 목록
  with the same id but is a different node instance :  path data 로 설정한 경로의 data 디렉토리를 지운다 
  failed to send join request to master : 
  not enough master nodes discovered during pinging : 한 서버 내에서 두개 이상의 노드를 올릴때, discovery.zen.ping.unicast.hosts 의 서버 ip 뒤에 포트번호도 붙여줘야함
   [ "203.236.86.169", "10.1.1.68"(해당서버) ] (X)
   [ "203.236.86.169", "10.1.1.68:9300" ]  (O)

해결 참고자료 : 
https://github.com/elastic/elasticsearch/issues/21405
https://github.com/elastic/elasticsearch/issues/15373
http://lng1982.tistory.com/302




◾Athena1 master node - elasticsearch yml



cluster.name: yj-elastic
node.name: athena1-masterNode
node.max_local_storage_nodes: 3
discovery.zen.ping.unicast.hosts: [ "10.1.1.68", "203.236.86.169" ]
network.host : 0.0.0.0
path.data: /data/wipsplus/ElasticSearch/elasticsearch-6.2.4/elastic/data/
node.master: true
node.data: false
http.port: 9200
transport.tcp.compress: true
transport.tcp.port: 9300


 

•Athena1 data node - elasticsearch yml



cluster.name: yj-elastic
node.name: athena1-dataNode
node.max_local_storage_nodes: 3
discovery.zen.ping.unicast.hosts: [ "203.236.86.169", "10.1.1.68:9300" ]
network.host : 0.0.0.0
path.data: /data/wipsplus/ElasticSearch/2/elastic/data/
node.master: false
node.data: true
http.port: 9201
transport.tcp.compress: true
transport.tcp.port: 9301


 


•Athena2 data node - elasticsearch yml


cluster.name: yj-elastic
node.name: athena2-dataNode
node.max_local_storage_nodes: 3
discovery.zen.ping.unicast.hosts: [ "203.236.86.168", "10.1.1.69" ]
network.host : 0.0.0.0
path.data: /data/wipsplus/ElasticSearch/2/elastic/data/
node.master: false
node.data: true
http.port: 9200
transport.tcp.compress: true
transport.tcp.port: 9300


 

마스터 노드는 명령 수행 전담 하는 창구 기능, 데이터 노드는 http 통신 기능을 막아 REST API 접근을 차단하고 데이터를 저장하는 역할

 


2 shards 설정

 
◾shard 분할 5개 / 복제본 1개로 색인 생성



mater-node에는 할당되지 않고, data-node 1, 2에 shard 5개 * 복제본 1개 추가 = 총 10개의 샤드로 구성 된다


◾number_of_replicas : 2 로 지정한채로 데이터 노드는 2개일 경우
● status가 green -> yellow 변경   
● 지정되지 않은 샤드 발견


3 shards 최적화

      ~일래스틱서치 고급 기능의 개념과 활용 - 6장 인덱스 배포 아키텍처~

•Elastic Search를 재기동 할 경우

 재배치(Reallocation) 하지 않고 재시작하기, Rolling Restarts

ElasticSearch의 색인 된 데이터는 N개의 노드에 임의로 분배하여 배치되어 있는데, 설정된 Shards(샤드) 수 만큼 분산저장되어 있다
그리고 다수의 노드일 경우 한 노드가 내려가게 되면 그 노드에 저장되어있는 Shard를 검색할 수 없기 때문에, 노드에 있던 Shard를 사용하기 위해서
내려간 노드를 제외하고 나머지 노드들끼리 샤드를 재분배하여 재 배치를 수행한다

이러한 작업은 많은 리소스를 발생시키고 운영중이 서버에서는 위험부담이 있으므로 재 배치를 하지 않도록 먼저 해당 노드에서 아래와 같은 쿼리를 먼저 실행하자.


# 재배치 중지 $ curl -XPUT 'localhost:9200/_cluster/settings?pretty=true' -d { "transient" : { "cluster.routing.allocation.enable" : "none" } }

재기동 / 환경설정 을 마쳐서 다시 노드를 올릴 경우


# 재배치 시작 $ curl -XPUT 'localhost:9200/_cluster/settings?pretty=true' -d { "transient" : { "cluster.routing.allocation.enable" : "all" } }

https://polbear.github.io/technical/information/2017/11/12/ElasticSerach-backup-restore.html
http://asuraiv.blogspot.com/2015/04/elasticsearch-rolling-restarts.html



 

•TIP: 작은 샤드들은 작은 세그먼트로 생성될 것이며, 오버헤드를 유발시킨다. 샤드의 크기는 수 GB ~ 수십 GB 수준으로 만들도록 노력할 것.
시간 기반 데이터(항공기 운항 기록 등)의 사용례에서는 흔히 20~40GB의 크기를 보였다.

 
•TIP: 샤드 당 오버헤드는 세그먼트 크기와 갯수에 의해 결정되므로,
작은 세그먼트들을 병합하도록 강제하는 forcemerge 연산은 오버헤드를 줄이고 질의 퍼포먼스를 향상시킬 수 있다.
이는 일단 색인 작업을 마쳐야 할 수 있으며, 비용이 높은 연산이므로 피크타임 이외에 수행되어야 할 것이다.

 
•TIP: 노드가 들고 있는 샤드의 갯수는 사용 가능한 heap에 의해 결정되겠지만, Elasticsearch 자체에는 한계값을 주고 있지는 않다.
좋은 방법은 노드 당 샤드의 갯수를 힙의 GB당 20~25개 정도를 지정하는 것이다.
즉 30GB 힙을 들고 있는 노드는 최대 600~750개의 샤드를 가질 수 있지만, 이것보다는 아래 값으로 지정하는 것이 좋을 것이다.
이렇게 하여 클러스터를 good health로 유지할 수 있을 것이다.

https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster

4 SnapShot 백업 및 복원

 

 

5. SnapShot 다른 클러스터로 복원 설정

특정 색인 시점을 백업 및 복구 하기 위한 SnapShot

백업/복원 순서 :
◾ 백업 경로 설정 ( 디렉토리 생성  )

◾ elasticsearch 설정파일에 경로 추가


◾서비스 재기동

◾ 저장소 설정 명령어


- 백업 할 저장소 옵션 설정


- 저장소 밑에 특정 색인 스냅샷 설정


◾ 백업 파일 배포
공유 파일 시스템 이든 scp 를 이용해서 다른 클러스터로 이동

> 복사 할 백업 대상 Athena1/backup


> 복사 될 곳 Athena2/backup


scp 나 rsync -avxH 를 이용하여 백업 색인 배포


◾elasticsearch 설정파일에 경로 추가 > 서비스 재기동
◾복구
replicase 는 복제본 설정 갯수 입니다 ( 설정 안할 시 기본값 1 )
 


◾복원 됬을 경우


다 완료 됬을경우 샤드5개 * (복제본(1)+기본(1)) 로 분포가 되는데, 실제 올라올때  Initializing으로 표기 되며, 노드가 1개고, 다른 노드가 올라오지도 않은 샤드들은 Unassigned Replica
로 표기 됩니다

 

6 replica 매핑 수정
PUT /twitter/_settings { "index" : { "number_of_replicas" : 2 } }

https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html

 

 

 
